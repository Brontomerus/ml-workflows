{
 "metadata": {
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import boto3\n",
    "\n",
    "from dask_cloudprovider.aws import FargateCluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "# modeling\n",
    "import dask_ml\n",
    "# from dask_ml. import "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining AWS ECS+Fargate Cluster\n",
    "We'll need to define our underlying resources where our code will run on. I'm using a temporary cluster, much like that which will run our managed work loads via prefect when automated. Here, we will define a cluster, spin up the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our model developement cluster\n",
    "cluster = FargateCluster(\n",
    "    n_workers = 4,\n",
    "    fargate_use_private_ip = False, # I don't really feel like going through the trouble of making this private\n",
    "    worker_cpu = 1024,\n",
    "    worker_mem = 4096,\n",
    "    cluster_arn = '',\n",
    "    vpc = '',\n",
    "    subnets = ['',''], # use the public subnets from creating them in the IaC\n",
    "    security_groups = ['',''] \n",
    "    scheduler_timeout = '15 minutes',\n",
    "    image = 'daskdev/dask:2021.2.0', \n",
    "    environment = {\n",
    "        'EXTRA_CONDA_PACKAGES': '',\n",
    "        'EXTRA_PIP_PACKAGES': 'dask-ml, boto3, '\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to create our dask.distributed Client object for establishing a connection to the Fargate Cluster we're spinning up. Assure this is closed when done using it with `cluster.close()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serializing Feature Processing & Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Model To Versioned S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  }
 ]
}